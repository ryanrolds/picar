{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Script to finetune AlexNet using Tensorflow.\n",
    "\n",
    "With this script you can finetune AlexNet as provided in the alexnet.py\n",
    "class on any given dataset. Specify the configuration settings at the\n",
    "beginning according to your problem.\n",
    "This script was written for TensorFlow >= version 1.2rc0 and comes with a blog\n",
    "post, which you can find here:\n",
    "\n",
    "https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html\n",
    "\n",
    "Author: Frederik Kratzert\n",
    "contact: f.kratzert(at)gmail.com\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from alexnet import AlexNet\n",
    "from datagenerator import ImageDataGenerator\n",
    "from datetime import datetime\n",
    "Iterator = tf.data.Iterator\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\"\"\"\n",
    "Configuration Part.\n",
    "\"\"\"\n",
    "\n",
    "# Path to the textfiles for the trainings and validation set\n",
    "train_file = 'train.txt'\n",
    "val_file = 'dev.txt'\n",
    "\n",
    "# Learning params\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Network params\n",
    "dropout_rate = 0.5\n",
    "num_classes = 6\n",
    "train_layers = ['fc8', 'fc7', 'fc6']\n",
    "\n",
    "# How often we want to write the tf.summary data to disk\n",
    "display_step = 100\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"tmp/finetune_alexnet/tensorboard\"\n",
    "checkpoint_path = \"tmp/finetune_alexnet/checkpoints\"\n",
    "#checkpoint_file = None\n",
    "#checkpoint_file = \"tmp/finetune_alexnet/checkpoints/model_epoch1012.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc6/weights:0/gradient is illegal; using fc6/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc6/biases:0/gradient is illegal; using fc6/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0/gradient is illegal; using fc7/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0/gradient is illegal; using fc7/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0/gradient is illegal; using fc8/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0/gradient is illegal; using fc8/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc6/weights:0 is illegal; using fc6/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc6/biases:0 is illegal; using fc6/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0 is illegal; using fc7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0 is illegal; using fc7/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0 is illegal; using fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0 is illegal; using fc8/biases_0 instead.\n",
      "INFO:tensorflow:Restoring parameters from tmp/finetune_alexnet/checkpoints/model_step_6072.ckpt-6072\n",
      "Current step 0\n",
      "2018-04-04 20:25:58.767526 Start training...\n",
      "2018-04-04 20:25:58.767570 Open Tensorboard at --logdir tmp/finetune_alexnet/tensorboard\n",
      "2018-04-04 20:25:58.767595 Epoch number: 1\n",
      "2018-04-04 20:33:30.241239 Start validation\n",
      "2018-04-04 20:34:11.925895 Validation Accuracy = 0.4828\n",
      "2018-04-04 20:34:13.544369 Model checkpoint saved at tmp/finetune_alexnet/checkpoints/model.ckpt\n",
      "2018-04-04 20:34:13.544465 Epoch number: 2\n",
      "2018-04-04 20:41:35.340210 Start validation\n",
      "2018-04-04 20:42:16.696277 Validation Accuracy = 0.4848\n",
      "2018-04-04 20:42:18.313730 Model checkpoint saved at tmp/finetune_alexnet/checkpoints/model.ckpt\n",
      "2018-04-04 20:42:18.313819 Epoch number: 3\n",
      "2018-04-04 20:49:40.985071 Start validation\n",
      "2018-04-04 20:50:22.679767 Validation Accuracy = 0.4800\n",
      "2018-04-04 20:50:24.455968 Model checkpoint saved at tmp/finetune_alexnet/checkpoints/model.ckpt\n",
      "2018-04-04 20:50:24.456085 Epoch number: 4\n",
      "2018-04-04 20:57:57.218580 Start validation\n",
      "2018-04-04 20:58:39.231941 Validation Accuracy = 0.4802\n",
      "2018-04-04 20:58:40.877179 Model checkpoint saved at tmp/finetune_alexnet/checkpoints/model.ckpt\n",
      "2018-04-04 20:58:40.877284 Epoch number: 5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main Part of the finetuning Script.\n",
    "\"\"\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)\n",
    "    \n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(filewriter_path):\n",
    "    os.mkdir(filewriter_path)\n",
    "    \n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(filewriter_path + \"/dev\"):\n",
    "    os.mkdir(filewriter_path + \"/dev\")\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(filewriter_path + \"/train\"):\n",
    "    os.mkdir(filewriter_path + \"/train\")\n",
    "\n",
    "# Place data loading and preprocessing on the cpu\n",
    "with tf.device('/CPU:0'):\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=True)\n",
    "    val_data = ImageDataGenerator(val_file,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                       tr_data.data.output_shapes)\n",
    "    next_batch = iterator.get_next()\n",
    "\n",
    "# Ops for initializing the two different iterators\n",
    "training_init_op = iterator.make_initializer(tr_data.data)\n",
    "validation_init_op = iterator.make_initializer(val_data.data)\n",
    "\n",
    "# TF placeholder for graph input and output\n",
    "x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Initialize model\n",
    "model = AlexNet(x, keep_prob, num_classes, train_layers)\n",
    "\n",
    "# Link variable to model output\n",
    "score = model.fc8\n",
    "\n",
    "# List of trainable variables of the layers we want to train\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=score, labels=y))\n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    # Get gradients of all trainable variables\n",
    "    gradients = tf.gradients(loss, var_list)\n",
    "    gradients = list(zip(gradients, var_list))\n",
    "\n",
    "    # Create optimizer and apply gradient descent to the trainable variables\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "# Add gradients to summary\n",
    "for gradient, var in gradients:\n",
    "    tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "# Add the variables we train to the summary\n",
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize the FileWriters\n",
    "trainWriter = tf.summary.FileWriter(filewriter_path + \"/train\")\n",
    "devWriter = tf.summary.FileWriter(filewriter_path + \"/dev\")\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Get the number of training/validation steps per epoch\n",
    "train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "val_batches_per_epoch = int(np.floor(val_data.data_size / batch_size))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "increment_global_step = tf.assign_add(global_step, 1, name = 'increment_global_step')\n",
    "\n",
    "# Start Tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Load the pretrained weights into the non-trainable layer\n",
    "    #if not checkpoint_file:\n",
    "    model.load_initial_weights(sess)\n",
    "\n",
    "    # Restore checkpoint\n",
    "    if checkpoint_path:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(checkpoint_path))\n",
    "        \n",
    "    currentStep = sess.run(global_step)\n",
    "    print(\"Current step\", currentStep)\n",
    "    \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), filewriter_path))\n",
    "\n",
    "    # Loop over number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "\n",
    "            # And run the training op\n",
    "            sess.run([train_op, increment_global_step],\n",
    "                     feed_dict={x: img_batch, y: label_batch, keep_prob: dropout_rate})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                currentStep = sess.run(global_step)\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch, y: label_batch, keep_prob: 1.})\n",
    "                trainWriter.add_summary(s, currentStep)\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        \n",
    "        sess.run(validation_init_op)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        \n",
    "        for _ in range(val_batches_per_epoch):\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch, y: label_batch, keep_prob: 1.})\n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "            \n",
    "        test_acc /= test_count\n",
    "        \n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(), test_acc))\n",
    "        \n",
    "        # Output summary to dev writer\n",
    "        currentStep = sess.run(global_step)\n",
    "        s = sess.run(merged_summary, feed_dict={x: img_batch, y: label_batch, keep_prob: 1.})\n",
    "        devWriter.add_summary(s, currentStep)\n",
    "        \n",
    "        # save checkpoint of the model\n",
    "        checkpoint_name = os.path.join(checkpoint_path, 'model.ckpt')\n",
    "        save_path = saver.save(sess, checkpoint_name, global_step=currentStep)\n",
    "\n",
    "        print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
